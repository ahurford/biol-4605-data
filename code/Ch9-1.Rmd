---
title: "Chapter 9.1: Linear regression explanatory variable fixed by experiment"
author: "Amy Hurford (ahurford@mun.ca)"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
---

Here, we show the R code to analyze the `Pcorn` data. We test the assumptions of the linear regression. As we find an assumption violation, we then perform a randomization to determine if the assumption violation matters.

This .Rmd file can be downloaded here: [https://github.com/ahurford/biol-4605-data/blob/main/code/Ch9-1.Rmd](https://github.com/ahurford/biol-4605-data/blob/main/code/Ch9-1.Rmd)


## Loading the data and doing the linear regression
```{r}
# This imports the data into R from a website without needing to download
# Generally, click 'Raw' on the github website and copy url
data <- read.csv('https://raw.githubusercontent.com/ahurford/biol-4605-data/main/data/corn.csv', fill=TRUE)

# Give the variables shorter names: 
# Response variable
Pcorn = data$Pcorn
# Explanatory variable
Psoil = data$Psoil

# Do the linear regression:
reg <- lm(Pcorn~Psoil)
# see the results of your regression
summary(reg)
```


```{r}
# Plot of the regression (line) and the data (open circles)
plot(Psoil,Pcorn)
abline(reg)
```

## Calculating the likelihood ratio
Calculate the likelihood ratio for the evidence supporting the linear regression relative to a null model that for all values of phosphorous in the soil (`Psoil`), the phosphorous in the corn (`Pcorn`) is predicted as the mean recorded value.

```{r}
# NULL MODEL
# A dataframe with the residuals and the residuals^2
# Model values are equal to mean(Pcorn)
data.eq_null = data.frame(Psoil = Psoil, Data = Pcorn, Model = rep(mean(Pcorn),length(Pcorn)), res = Pcorn-mean(Pcorn), res2 = (Pcorn-mean(Pcorn))^2)
# Print this dataframe with the residuals and the residuals^2 for the null model
data.eq_null
```


```{r}
#The sum of the residuals should be 0.
# SS.total is the total sum squares
sum(data.eq_null$res)
SS.total = sum(data.eq_null$res2)
SS.total #prints the value
```


```{r}
# FULL MODEL
# Similar to the data frame for the null model, but for the full model the model values are equal to reg$fitted.values (were reg is the linear regression model object)
data.eq_full = data.frame(Psoil = Psoil, Data = Pcorn, Model = reg$fitted.values, res = Pcorn-reg$fitted.values, res2 = (Pcorn-reg$fitted.values)^2)
# Print this dataframe with the residuals and the residuals^2 for the regression model
data.eq_full
```
```{r}
#The sum of the residuals should be 0.
#SS.res is the sum squares of the residuals
sum(data.eq_full$res)
SS.res = sum(data.eq_full$res2)
SS.res #prints the value
```

```{r}
# Calculate the Likelihood ratio
n = length(Pcorn)
LR = (SS.res/SS.total)^(-n/2)
LR # prints
```

## Model assumptions
The plot function `plot()` applied to a model object (here `reg`) prints a series of graphs that visualize the agreement of the fitted model with the assumptions of the linear regression.
```{r}
# A series of plots to check model assumptions
plot(reg)
```


The Q-Q plot shows a poor agreement of the linear model residuals relative to a normal distribution (an assumption of the general linear model). In particular, observations 3 and 7 are much further from the model predicted values than would be expected if the residuals were normally distributed.

## Randomization
Does this possible violation of the model assumptions change our conclusion that there is a relationship between the phosphorous in the soil and in the corn? 

```{r, include=F}
p.val = anova(lm(Pcorn~Psoil))$`Pr(>F)`[1]
```

The regression slope coefficient was `r round(coef(reg)[2],2)` with a p-value of `r round(p.val,1)` suggesting that the probability of observing these data, if there truly was no relationship between the phosphorous in the soil and in the corn (i.e., a Type II error) is `r round(10000*p.val,1)` out of 10000  *given that the assumptions of the general linear model are met*.

From our Q-Q plot, we know that the assumptions of the general linear model are not met, but does this matter?

To test this, we randomly assign values of `Pcorn` to values of `Psoil` as new hypothetical data. Conceptually, if there really is no relationship between `Psoil` and `Pcorn`, then any of the `Pcorn` values could have been reported for any of the `Psoil` measurements. How does an F-value calculated for the observed data, compare with an F-value calculated for data where `Pcorn` values are randomly assigned to any `Psoil` measurement?

The function below completes 10,000 replicates of hypothetical data that might be observed if there was truly no effect of `Psoil`: this is `sample(y,length(y),TRUE)`. Here `y` is the response variable supplied to the function, and we sample from `y` to make new response variable data of length `length(y)`. The sample is performed with replacement (`=TRUE`).

The explanatory variable (in the function is `x`). In our application, this will be `Psoil`, and this remains the same for all the linear regressions with the randomly associated response variable data. From the regression, we extract the F-value with `$`F value``.

Below is a function, `F.rand()`, that is defined for any supplied `x` and `y`.

```{r}
F.rand = function(x,y){
result = replicate(10000,anova(lm(sample(y,length(y),TRUE)~x))$`F value`[1])
}
```

Note that randomly choosing an observed `Pcorn` value to be associated with an observed `Psoil` value makes different assumptions than the assumptions of the general linear model, so this approach serves to test the robustness of our conclusions to the assumptions of the general linear model (which our error diagnostic plots showed to be violated).

```{r}
# We use the function F.rand() and calculate 10,000 F-values for random associations between Pcorn and Psoil.
rand = F.rand(Psoil,Pcorn)
# Prints the F-values for the first 6 random data sets:
head(rand)
```

The F-value that we calculated from the data was quite a bit larger than the first 6 F-values for the data generated by random associations between the explanatory and response variables:
```{r}
F.data <- anova(lm(Pcorn~Psoil))$`F value`[1]
F.data # prints
```

How many F-values from the randomizations are bigger than the F-value for the real data? We calculate this as:

```{r}
n.rand = length(which(rand>F.data))
n.rand #prints
```
Under the assumptions of the general linear model, we expected failure to reject the null hypothesis `r round(10000*p.val,1)` times out of 10,000. For the different assumptions of the randomization, we failed to reject the null hypothesis `r n.rand` times out of 10,000.

However, in either case, it is extremely unlikely that there is no relationship between `Psoil` and `Pcorn`: for the randomization this occurs `r n.rand/100`% of the time. Therefore, the violation of the general linear model assumptions, mean that we might not be able to interpret the reported p-vale as the probability of the Type II error, however, the conclusion that there is a relationship between `Psoil` and `Pcorn` is justified, even given the assumption violation shown in the QQ-plot. This result makes sense, since the likelihood ratio (the calculated value was `r round(LR,1)`) suggested very strong support for this relationship also.
